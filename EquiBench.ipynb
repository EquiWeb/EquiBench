{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EquiBench\n",
    "Coding benchmark for evaluating the performance of LLMs in both unserstanding and generating code. Specifically benchmark focuses on performance of LLMs as coding agents within large codebases, and their ability to understand and generate code in a way that is consistent and compatible with the existing codebase.\n",
    "v1 React repo; PR 32224 (this is probbably in the training set, but proves the concept and provides a starting point for future iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8a7ca8cf-bc2f-493b-8a6e-2bbdbbde866e",
    "_uuid": "ea6873f8-8967-4e96-bba3-e26ad4428113",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import kaggle_benchmarks as kbench\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Benchmark metadata\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "TASK_NAME = \"react_fiber_suspense_hydration_anywhere_pr_32224\"\n",
    "REPO_PATH = \"./react\"\n",
    "PR_NUMBER = 32224\n",
    "REPO_SLUG  = \"facebook/react\"\n",
    "\n",
    "PR_SUMMARY = \"\"\"\n",
    "[Fiber] support hydration when rendering Suspense anywhere #32224\n",
    "\n",
    "**Description:**\n",
    "This PR addresses a long-standing issue where React could not correctly hydrate a\n",
    "tree that contained a Suspense boundary if the boundary was not also present on the\n",
    "server-rendered HTML. This would often lead to the entire tree being de-optimized\n",
    "and client-rendered from scratch.\n",
    "\n",
    "The core change is to allow the hydration process to \"skip over\" unexpected Suspense\n",
    "boundaries found on the client. Instead of throwing an error, React will now treat\n",
    "the content inside the Suspense boundary as a client-only insertion.\n",
    "\n",
    "**Key Changes:**\n",
    "1. **`react-reconciler`:**\n",
    "   - Modified `hydrateSuspenseBoundary` and related functions in\n",
    "     `ReactFiberHydrationContext.js`.\n",
    "   - Introduced logic to handle cases where a Suspense boundary is encountered\n",
    "     during hydration but no corresponding server-rendered content exists.\n",
    "   - The reconciler now attempts to hydrate the children of the Suspense boundary.\n",
    "     If it can't find matching DOM nodes, it switches to client-rendering mode for\n",
    "     that subtree.\n",
    "2. **Fiber Flags:**\n",
    "   - New or modified flags to track the hydration state of Suspense boundaries.\n",
    "3. **Testing:**\n",
    "   - New test cases in `ReactSuspenseHydration-test.js` covering these hydration\n",
    "     scenarios, including nested Suspense boundaries and client-only boundaries.\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Ground-truth fallbacks\n",
    "# These are documented here for reference and used as a sanity check.\n",
    "# The authoritative GT_FILES / GT_CONTENTS / PR_TEST_FILES are computed\n",
    "# dynamically in Phase 1 by diffing the actual PR against its merge base.\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "FALLBACK_GT_FILES = {\n",
    "    \"packages/react-reconciler/src/ReactFiberHydrationContext.js\",\n",
    "    \"packages/react-reconciler/src/ReactFiberWorkLoop.js\",\n",
    "    \"packages/react-reconciler/src/__tests__/ReactSuspenseHydration-test.js\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_command(command: str, cwd: str, timeout: int = 600) -> subprocess.CompletedProcess:\n",
    "    \"\"\"Run a shell command and return the CompletedProcess result.\"\"\"\n",
    "    try:\n",
    "        return subprocess.run(\n",
    "            command,\n",
    "            cwd=cwd,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            shell=True,\n",
    "            timeout=timeout,\n",
    "            check=False,\n",
    "        )\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return subprocess.CompletedProcess(\n",
    "            args=command,\n",
    "            returncode=1,\n",
    "            stdout=\"\",\n",
    "            stderr=f\"Command timed out after {timeout} seconds.\",\n",
    "        )\n",
    "\n",
    "\n",
    "def get_git_modified_files(repo_path: str) -> set[str]:\n",
    "    \"\"\"Return the set of files modified (vs HEAD) according to git.\"\"\"\n",
    "    result = run_command(\"git diff --name-only HEAD\", cwd=repo_path, timeout=30)\n",
    "    if result.returncode != 0:\n",
    "        return set()\n",
    "    return {line.strip() for line in result.stdout.splitlines() if line.strip()}\n",
    "\n",
    "\n",
    "def strip_code_fences(text: str) -> str:\n",
    "    \"\"\"Remove opening and closing markdown code fences from LLM output.\"\"\"\n",
    "    text = re.sub(r\"^```[a-zA-Z]*\\n\", \"\", text.strip())\n",
    "    text = re.sub(r\"\\n```$\", \"\", text.strip())\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "Both phases run inside an agent loop and share the same tool set.  Tools are\n",
    "scoped to the repository root so the model cannot escape it.\n",
    "\n",
    "| Tool | Purpose |\n",
    "|------|---------|\n",
    "| `read_file` | Read a source file by repo-relative path |\n",
    "| `search_in_file` | Grep for a regex pattern within a single file |\n",
    "| `grep_repo` | Grep for a regex pattern across the entire repo (respects `.gitignore`) |\n",
    "| `list_directory` | List entries in a repo-relative directory |\n",
    "| `write_file` | Write (create or overwrite) a file — *implementation phase only* |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum characters returned by read_file tool — prevents context explosions on\n",
    "# large source files while still giving the model enough to work with.\n",
    "MAX_TOOL_FILE_CHARS = 12_000\n",
    "\n",
    "\n",
    "def make_repo_tools(repo_path: str, *, allow_writes: bool = False) -> list:\n",
    "    \"\"\"\n",
    "    Return a list of plain Python tool functions scoped to `repo_path`.\n",
    "\n",
    "    Pass allow_writes=True for the implementation phase; keep it False for the\n",
    "    read-only analysis/planning phase.\n",
    "\n",
    "    Tools are plain functions — no decorator needed; they are passed directly\n",
    "    to llm.prompt(tools=[...]).\n",
    "    \"\"\"\n",
    "    root = Path(repo_path).resolve()\n",
    "\n",
    "    def _safe_path(relative: str) -> Path:\n",
    "        \"\"\"Resolve a repo-relative path and reject path-escape attempts.\"\"\"\n",
    "        resolved = (root / relative).resolve()\n",
    "        if not str(resolved).startswith(str(root)):\n",
    "            raise ValueError(f\"Path '{relative}' escapes the repository root.\")\n",
    "        return resolved\n",
    "\n",
    "    def read_file(path: str) -> str:\n",
    "        \"\"\"\n",
    "        Read the contents of a file in the repository.\n",
    "\n",
    "        Args:\n",
    "            path: Repo-relative path, e.g.\n",
    "                  \"packages/react-reconciler/src/ReactFiberHydrationContext.js\"\n",
    "\n",
    "        Returns the file contents as a string, truncated to 12,000 characters\n",
    "        for large files (a truncation notice is appended when this occurs).\n",
    "        Raises FileNotFoundError if the file does not exist.\n",
    "        \"\"\"\n",
    "        text = _safe_path(path).read_text(encoding=\"utf-8\")\n",
    "        if len(text) > MAX_TOOL_FILE_CHARS:\n",
    "            notice = (\n",
    "                f\"\\n\\n[TRUNCATED: file is {len(text):,} chars; \"\n",
    "                f\"only the first {MAX_TOOL_FILE_CHARS:,} are shown. \"\n",
    "                \"Use search_in_file or grep_repo to locate specific sections.]\"\n",
    "            )\n",
    "            return text[:MAX_TOOL_FILE_CHARS] + notice\n",
    "        return text\n",
    "\n",
    "    def search_in_file(path: str, pattern: str) -> str:\n",
    "        \"\"\"\n",
    "        Search for a regex pattern within a single file, returning matching lines\n",
    "        with their line numbers.\n",
    "\n",
    "        Args:\n",
    "            path: Repo-relative path to the file.\n",
    "            pattern: Python regex pattern to search for.\n",
    "        \"\"\"\n",
    "        text = _safe_path(path).read_text(encoding=\"utf-8\")\n",
    "        matches = [\n",
    "            f\"L{i+1}: {line}\"\n",
    "            for i, line in enumerate(text.splitlines())\n",
    "            if re.search(pattern, line)\n",
    "        ]\n",
    "        return \"\\n\".join(matches) if matches else f\"No matches for '{pattern}' in {path}.\"\n",
    "\n",
    "    def grep_repo(pattern: str, file_glob: str = \"**/*.js\") -> str:\n",
    "        \"\"\"\n",
    "        Search for a regex pattern across all files matching a glob in the repo.\n",
    "\n",
    "        Args:\n",
    "            pattern: Python regex pattern to search for.\n",
    "            file_glob: Glob pattern relative to repo root. Default: \"**/*.js\".\n",
    "\n",
    "        Returns up to 50 matching lines with file paths and line numbers.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for filepath in root.glob(file_glob):\n",
    "            if \".git\" in filepath.parts:\n",
    "                continue\n",
    "            try:\n",
    "                for i, line in enumerate(\n",
    "                    filepath.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
    "                ):\n",
    "                    if re.search(pattern, line):\n",
    "                        rel = str(filepath.relative_to(root))\n",
    "                        results.append(f\"{rel}:L{i+1}: {line}\")\n",
    "                        if len(results) >= 50:\n",
    "                            results.append(\"... (truncated at 50 matches)\")\n",
    "                            return \"\\n\".join(results)\n",
    "            except (PermissionError, IsADirectoryError):\n",
    "                continue\n",
    "        return \"\\n\".join(results) if results else f\"No matches for '{pattern}'.\"\n",
    "\n",
    "    def list_directory(path: str = \"\") -> str:\n",
    "        \"\"\"\n",
    "        List the contents of a directory in the repository.\n",
    "\n",
    "        Args:\n",
    "            path: Repo-relative directory path (empty string = repo root).\n",
    "\n",
    "        Returns a newline-separated list of entries; directories are suffixed '/'.\n",
    "        \"\"\"\n",
    "        target = _safe_path(path) if path else root\n",
    "        if not target.is_dir():\n",
    "            return f\"'{path}' is not a directory.\"\n",
    "        entries = sorted(target.iterdir(), key=lambda p: (p.is_file(), p.name))\n",
    "        return \"\\n\".join((e.name + \"/\" if e.is_dir() else e.name) for e in entries)\n",
    "\n",
    "    tools = [read_file, search_in_file, grep_repo, list_directory]\n",
    "\n",
    "    if allow_writes:\n",
    "        def write_file(path: str, content: str) -> str:\n",
    "            \"\"\"\n",
    "            Write (create or overwrite) a file in the repository.\n",
    "\n",
    "            Args:\n",
    "                path: Repo-relative path of the file to write.\n",
    "                content: Complete new file content. Do not include markdown code\n",
    "                         fences — they will be stripped automatically.\n",
    "\n",
    "            Returns a confirmation message with the path and byte count written.\n",
    "            \"\"\"\n",
    "            target = _safe_path(path)\n",
    "            target.parent.mkdir(parents=True, exist_ok=True)\n",
    "            cleaned = strip_code_fences(content)\n",
    "            target.write_text(cleaned, encoding=\"utf-8\")\n",
    "            return f\"Written: {path} ({len(cleaned):,} chars)\"\n",
    "\n",
    "        tools.append(write_file)\n",
    "\n",
    "    return tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Tool Loop\n",
    "\n",
    "`llm.prompt(tools=...)` with automatic tool-calling only works when the model is\n",
    "loaded with `api=\"genai\"` (Google GenAI).  For all other providers and APIs —\n",
    "OpenAI-compatible, Anthropic, open-weights models, etc. — tools must be driven\n",
    "manually.\n",
    "\n",
    "`run_tool_loop()` implements a **provider-agnostic agentic loop** that works\n",
    "identically across every model and provider kbench supports:\n",
    "\n",
    "1. The system prompt describes available tools in plain text and instructs the\n",
    "   model to emit a single JSON object `{\"tool\": \"...\", \"args\": {...}}` when it\n",
    "   wants to call a tool, or plain text when it is done.\n",
    "2. Responses are inspected for a JSON tool-call object.  If found, the tool is\n",
    "   executed and the result is fed back via `llm.send()` + `llm.respond()`.\n",
    "3. The loop continues until the model produces a plain-text final response or\n",
    "   `max_iterations` is exhausted (which is recorded as a model failure).\n",
    "4. All turns occur inside a single `kbench.chats.new()` context so the model\n",
    "   has full conversation history throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import json\n",
    "\n",
    "# Regex to detect a JSON tool-call object anywhere in a model response.\n",
    "# Matches the outermost {...} that contains a \"tool\" key.\n",
    "_TOOL_CALL_RE = re.compile(r'\\{\\s*\"tool\"\\s*:', re.DOTALL)\n",
    "\n",
    "\n",
    "def _format_tool_descriptions(tools: list) -> str:\n",
    "    \"\"\"\n",
    "    Build a plain-text listing of available tools to embed in a prompt.\n",
    "\n",
    "    Uses each function's name, docstring, and type-annotated signature so the\n",
    "    model knows exactly what arguments to pass.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    for fn in tools:\n",
    "        sig = inspect.signature(fn)\n",
    "        params = []\n",
    "        for name, param in sig.parameters.items():\n",
    "            ann = param.annotation\n",
    "            type_str = ann.__name__ if hasattr(ann, \"__name__\") else str(ann)\n",
    "            if type_str == \"_empty\":\n",
    "                type_str = \"str\"\n",
    "            default = f\" = {param.default!r}\" if param.default is not inspect.Parameter.empty else \"\"\n",
    "            params.append(f\"{name}: {type_str}{default}\")\n",
    "        signature = f\"{fn.__name__}({', '.join(params)})\"\n",
    "        doc = (inspect.getdoc(fn) or \"\").strip()\n",
    "        lines.append(f\"### `{signature}`\\n{doc}\\n\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def _extract_tool_call(response: str) -> dict | None:\n",
    "    \"\"\"\n",
    "    Try to find and parse a JSON tool-call object from a model response.\n",
    "\n",
    "    Returns a dict with keys 'tool' and 'args', or None if the response is a\n",
    "    plain-text final answer (no tool call found or JSON is malformed).\n",
    "    \"\"\"\n",
    "    match = _TOOL_CALL_RE.search(response)\n",
    "    if not match:\n",
    "        return None\n",
    "    # Scan forward from the opening brace to find the matching closing brace.\n",
    "    start = match.start()\n",
    "    depth = 0\n",
    "    for i, ch in enumerate(response[start:], start):\n",
    "        if ch == \"{\":\n",
    "            depth += 1\n",
    "        elif ch == \"}\":\n",
    "            depth -= 1\n",
    "            if depth == 0:\n",
    "                candidate = response[start : i + 1]\n",
    "                try:\n",
    "                    parsed = json.loads(candidate)\n",
    "                    if \"tool\" in parsed:\n",
    "                        return parsed\n",
    "                except json.JSONDecodeError:\n",
    "                    return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def run_tool_loop(\n",
    "    llm,\n",
    "    prompt: str,\n",
    "    tools: list,\n",
    "    max_iterations: int,\n",
    "    chat_name: str,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Provider-agnostic agentic tool-calling loop.\n",
    "\n",
    "    Works with every model and API supported by kbench — no `api=\"genai\"` required.\n",
    "\n",
    "    The loop sends `prompt` as the first message.  If the model emits a JSON\n",
    "    tool-call object, the named tool is executed and its result is fed back as\n",
    "    a user message.  This continues until the model gives a plain-text final\n",
    "    response or `max_iterations` is reached.\n",
    "\n",
    "    Args:\n",
    "        llm:            The kbench LLM object for the model under evaluation.\n",
    "        prompt:         The full initial prompt, including embedded tool descriptions.\n",
    "        tools:          List of plain Python tool functions.\n",
    "        max_iterations: Maximum tool-call turns before declaring a model failure.\n",
    "        chat_name:      Name passed to kbench.chats.new() for context isolation.\n",
    "\n",
    "    Returns:\n",
    "        The model's final plain-text response.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If max_iterations is exhausted without a plain-text response.\n",
    "                      Callers should catch this and call kbench.assertions.assert_fail().\n",
    "    \"\"\"\n",
    "    tool_map = {fn.__name__: fn for fn in tools}\n",
    "\n",
    "    with kbench.chats.new(chat_name):\n",
    "        response = llm.prompt(prompt)\n",
    "\n",
    "        for iteration in range(max_iterations):\n",
    "            tool_call = _extract_tool_call(response)\n",
    "\n",
    "            if tool_call is None:\n",
    "                # Model gave a plain-text response — treat as final answer.\n",
    "                return response\n",
    "\n",
    "            tool_name = tool_call.get(\"tool\", \"\")\n",
    "            tool_args = tool_call.get(\"args\", {})\n",
    "\n",
    "            if tool_name not in tool_map:\n",
    "                tool_result = (\n",
    "                    f\"Error: unknown tool '{tool_name}'. \"\n",
    "                    f\"Available tools: {sorted(tool_map.keys())}\"\n",
    "                )\n",
    "            else:\n",
    "                try:\n",
    "                    tool_result = str(tool_map[tool_name](**tool_args))\n",
    "                except Exception as e:\n",
    "                    tool_result = f\"Error executing {tool_name}({tool_args}): {type(e).__name__}: {e}\"\n",
    "\n",
    "            print(f\"  [{iteration + 1}/{max_iterations}] {tool_name} → {tool_result[:120]}...\")\n",
    "\n",
    "            # Feed the result back and get the next model turn.\n",
    "            llm.send(f\"Tool result ({tool_name}):\\n{tool_result}\")\n",
    "            response = llm.respond()\n",
    "\n",
    "        # After the final iteration, check once more before raising — the model\n",
    "        # may have produced a plain-text answer on the last turn.\n",
    "        if _extract_tool_call(response) is None:\n",
    "            return response\n",
    "\n",
    "        raise RuntimeError(\n",
    "            f\"Tool loop exhausted {max_iterations} iterations without producing \"\n",
    "            \"a final plain-text response. The model may be stuck in a tool-calling loop.\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts\n",
    "\n",
    "Both phases use `run_tool_loop()` — the provider-agnostic loop defined above —\n",
    "so the benchmark works identically across every model and API.\n",
    "\n",
    "Tool descriptions are embedded directly into the prompt as plain text, and the\n",
    "model is instructed to emit a structured JSON object when calling a tool:\n",
    "\n",
    "```\n",
    "{\"tool\": \"<name>\", \"args\": {\"<param>\": \"<value>\", ...}}\n",
    "```\n",
    "\n",
    "Plain-text output (no JSON tool-call object) signals the final answer.\n",
    "\n",
    "### Planning phase (judge LLM — read-only tools)\n",
    "The judge sees the PR summary and file list, then must actively explore the\n",
    "codebase using tools before writing its plan.  No file content is pre-loaded —\n",
    "the judge must discover the relevant code itself.\n",
    "\n",
    "### Implementation phase (agent LLM — read + write tools)\n",
    "The agent receives the plan and has both read tools and `write_file`.  Because\n",
    "all turns occur in a single `kbench.chats.new()` context, the model has full\n",
    "history of every tool call and response — it knows what it has already read or\n",
    "written without any external bookkeeping.\n",
    "\n",
    "### Failure conditions\n",
    "Exhausting `max_iterations`, a context-limit error, or any other exception is\n",
    "caught and recorded as a model failure via `assert_fail`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent loop budgets — exhausting these is treated as a model failure.\n",
    "MAX_ANALYSIS_ITERATIONS = 30   # judge: explore repo + write plan\n",
    "MAX_IMPL_ITERATIONS     = 60   # agent: read + write all files\n",
    "\n",
    "# Tool-call instruction block injected at the top of every agent prompt.\n",
    "_TOOL_INSTRUCTIONS = \"\"\"\\\n",
    "## How to Use Tools\n",
    "You operate in a tool-calling loop. At each step you may either:\n",
    "\n",
    "**Call a tool** — respond with ONLY a JSON object in this exact format:\n",
    "{{\"tool\": \"<tool_name>\", \"args\": {{\"<param>\": \"<value>\"}}}}\n",
    "\n",
    "Example:\n",
    "{{\"tool\": \"read_file\", \"args\": {{\"path\": \"packages/react-reconciler/src/ReactFiberHydrationContext.js\"}}}}\n",
    "\n",
    "Rules:\n",
    "- Emit ONLY the JSON object when calling a tool — no text before or after it.\n",
    "- Call one tool at a time. Wait for the result before calling the next.\n",
    "- Use the exact tool names listed under \"Available Tools\" below.\n",
    "\n",
    "**Give your final response** — when you are done calling tools, respond with\n",
    "plain text (no JSON tool-call object). This plain-text response is what gets\n",
    "recorded as your answer.\n",
    "\n",
    "## Available Tools\n",
    "{tool_descriptions}\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def build_analysis_prompt(pr_summary: str, files_to_modify: list[str], tools: list) -> str:\n",
    "    \"\"\"\n",
    "    Prompt for the judge LLM's planning phase.\n",
    "\n",
    "    Tool descriptions are embedded directly so the model knows what it can call.\n",
    "    No file content is pre-loaded — the judge must explore the codebase itself.\n",
    "    \"\"\"\n",
    "    file_list = \"\\n\".join(f\"  - {f}\" for f in sorted(files_to_modify))\n",
    "    tool_descriptions = _format_tool_descriptions(tools)\n",
    "    tool_header = _TOOL_INSTRUCTIONS.format(tool_descriptions=tool_descriptions)\n",
    "\n",
    "    return f\"\"\"{tool_header}You are a senior React core team member producing an \\\n",
    "implementation plan for a Pull Request.\n",
    "\n",
    "## What You Must Do\n",
    "1. **Explore the codebase first.** Read the files listed below and grep for \\\n",
    "relevant symbols (e.g. `hydrateSuspenseBoundary`, `SuspenseState`, fiber flags). \\\n",
    "Do not rely on the PR summary alone — look at the actual code.\n",
    "2. **Produce a precise implementation plan.** For each file, describe:\n",
    "   - *Why* it needs to change, grounded in what you actually read.\n",
    "   - *What* to change — specific function names, new logic, new flags, new tests.\n",
    "   - *How* — key algorithmic steps, referencing real symbol names you found.\n",
    "\n",
    "## PR Summary\n",
    "{pr_summary}\n",
    "\n",
    "## Files to Modify\n",
    "{file_list}\n",
    "\n",
    "## Completion\n",
    "When you have read enough to write a specific, grounded plan, output it as \\\n",
    "plain text (no JSON tool-call object). Do not write any code.\"\"\"\n",
    "\n",
    "\n",
    "def build_implementation_prompt(\n",
    "    pr_summary: str,\n",
    "    implementation_plan: str,\n",
    "    files_to_modify: list[str],\n",
    "    tools: list,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Prompt for the agent LLM's implementation phase.\n",
    "\n",
    "    The agent has read + write tools and operates in a single continuous session,\n",
    "    so it sees the full history of everything it has already read or written.\n",
    "    \"\"\"\n",
    "    file_list = \"\\n\".join(f\"  - {f}\" for f in sorted(files_to_modify))\n",
    "    tool_descriptions = _format_tool_descriptions(tools)\n",
    "    tool_header = _TOOL_INSTRUCTIONS.format(tool_descriptions=tool_descriptions)\n",
    "\n",
    "    return f\"\"\"{tool_header}You are an expert React core contributor implementing \\\n",
    "a Pull Request.\n",
    "\n",
    "## What You Must Do\n",
    "1. **Read before writing.** Use `read_file` to inspect each file before modifying \\\n",
    "it. Use `grep_repo` or `search_in_file` to locate symbols referenced in the plan.\n",
    "2. **Follow the plan exactly.** Implement every change across all listed files.\n",
    "3. **Write complete files.** When calling `write_file`, pass the *entire* updated \\\n",
    "file — not a diff or partial snippet. Do not include markdown code fences.\n",
    "4. **Stay consistent.** You can re-read files you have already written to verify \\\n",
    "consistency before writing the next one.\n",
    "\n",
    "## PR Summary\n",
    "{pr_summary}\n",
    "\n",
    "## Implementation Plan\n",
    "{implementation_plan}\n",
    "\n",
    "## Files to Modify\n",
    "{file_list}\n",
    "\n",
    "## Completion\n",
    "Call `write_file` for every file in the list above. Once all files are written, \\\n",
    "output a plain-text summary of which files you modified (no JSON tool-call object).\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Phase\n",
    "\n",
    "Both phases call `run_tool_loop()`, which drives the tool-calling conversation\n",
    "manually using `llm.prompt()` → `llm.send()` → `llm.respond()`.  This works\n",
    "identically across every model and provider — no `api=\"genai\"` required.\n",
    "\n",
    "All turns occur within a single `kbench.chats.new()` context, so the model\n",
    "retains full history of every tool call and result for the duration of the phase.\n",
    "\n",
    "Any exception (context limit, API error, tool error) is caught and recorded as\n",
    "an explicit benchmark failure, not an infrastructure crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analysis_phase(judge_llm, repo_path: str, gt_files: set[str]) -> str:\n",
    "    \"\"\"\n",
    "    Phase 2: judge LLM explores the codebase and produces an implementation plan.\n",
    "\n",
    "    Uses run_tool_loop() — works across all model providers and APIs.\n",
    "    After the loop, assess_response_with_judge evaluates whether the plan is\n",
    "    grounded in actual code rather than just restating the PR summary.\n",
    "\n",
    "    Args:\n",
    "        judge_llm:  The fixed judge LLM (kbench.judge_llm).\n",
    "        repo_path:  Path to the React repo checked out at the merge-base commit.\n",
    "        gt_files:   Set of files changed by the PR — computed dynamically in Phase 1.\n",
    "\n",
    "    Returns the implementation plan as a string.\n",
    "    \"\"\"\n",
    "    print(\"PHASE 2: Analysis — judge LLM exploring codebase and creating plan.\")\n",
    "\n",
    "    tools = make_repo_tools(repo_path, allow_writes=False)\n",
    "    prompt = build_analysis_prompt(\n",
    "        pr_summary=PR_SUMMARY,\n",
    "        files_to_modify=sorted(gt_files),\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "    plan = None\n",
    "    try:\n",
    "        plan = run_tool_loop(\n",
    "            llm=judge_llm,\n",
    "            prompt=prompt,\n",
    "            tools=tools,\n",
    "            max_iterations=MAX_ANALYSIS_ITERATIONS,\n",
    "            chat_name=\"analysis_phase\",\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        kbench.assertions.assert_fail(\n",
    "            f\"Judge LLM failed to complete the analysis phase: {e}\"\n",
    "        )\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        kbench.assertions.assert_fail(\n",
    "            f\"Judge LLM errored during the analysis phase: {type(e).__name__}: {e}\"\n",
    "        )\n",
    "        return \"\"\n",
    "\n",
    "    kbench.assertions.assert_not_empty(\n",
    "        plan,\n",
    "        expectation=\"Judge LLM must produce a non-empty implementation plan.\",\n",
    "    )\n",
    "\n",
    "    gt_file_list = \", \".join(sorted(gt_files))\n",
    "    assess_report = kbench.assertions.assess_response_with_judge(\n",
    "        criteria=(\n",
    "            \"The plan references specific JavaScript function names or variable names \"\n",
    "            \"that would only be known from reading the actual source files \"\n",
    "            \"(e.g. hydrateSuspenseBoundary, SuspenseState, ReactFiberWorkLoop internals), \"\n",
    "            \"not just terms mentioned in the PR summary.\",\n",
    "            f\"The plan describes concrete, file-specific changes for each of the \"\n",
    "            f\"following files: {gt_file_list} — including which functions to modify \"\n",
    "            \"and the new logic to introduce.\",\n",
    "            \"The plan describes specific new test cases covering the hydration scenarios, \"\n",
    "            \"including what inputs and expected outcomes each test exercises.\",\n",
    "            \"The plan is actionable: a developer could implement the changes described \"\n",
    "            \"without needing to read the PR summary again.\",\n",
    "        ),\n",
    "        response_text=plan,\n",
    "        judge_llm=judge_llm,\n",
    "    )\n",
    "    for criterion_result in assess_report.results:\n",
    "        kbench.assertions.assert_true(\n",
    "            criterion_result.passed,\n",
    "            expectation=(\n",
    "                f\"Plan quality — {criterion_result.criterion} | \"\n",
    "                f\"Judge reasoning: {criterion_result.reason}\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    print(f\"Analysis complete — plan received ({len(plan)} chars).\")\n",
    "    return plan\n",
    "\n",
    "\n",
    "def run_implementation_phase(\n",
    "    llm,\n",
    "    judge_llm,\n",
    "    implementation_plan: str,\n",
    "    repo_path: str,\n",
    "    gt_files: set[str],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Phase 3: agent LLM reads and writes all gt_files using the tool loop.\n",
    "\n",
    "    Uses run_tool_loop() — works across all model providers and APIs.\n",
    "    Full conversation history is preserved throughout the session, so the model\n",
    "    is aware of every file it has already read or written.\n",
    "\n",
    "    Args:\n",
    "        llm:                 The model under evaluation.\n",
    "        judge_llm:           Used for assess_response_with_judge checks.\n",
    "        implementation_plan: The plan produced by run_analysis_phase.\n",
    "        repo_path:           Path to the React repo at the merge-base commit.\n",
    "        gt_files:            Set of files the agent must modify — from Phase 1 diff.\n",
    "    \"\"\"\n",
    "    print(\"PHASE 3: Implementation — agent LLM modifying files.\")\n",
    "\n",
    "    tools = make_repo_tools(repo_path, allow_writes=True)\n",
    "    prompt = build_implementation_prompt(\n",
    "        pr_summary=PR_SUMMARY,\n",
    "        implementation_plan=implementation_plan,\n",
    "        files_to_modify=sorted(gt_files),\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "    completion_response = None\n",
    "    try:\n",
    "        completion_response = run_tool_loop(\n",
    "            llm=llm,\n",
    "            prompt=prompt,\n",
    "            tools=tools,\n",
    "            max_iterations=MAX_IMPL_ITERATIONS,\n",
    "            chat_name=\"implementation_phase\",\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        kbench.assertions.assert_fail(\n",
    "            f\"Agent LLM failed to complete the implementation phase: {e}\"\n",
    "        )\n",
    "        return\n",
    "    except Exception as e:\n",
    "        kbench.assertions.assert_fail(\n",
    "            f\"Agent LLM errored during the implementation phase: {type(e).__name__}: {e}\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    kbench.assertions.assert_not_empty(\n",
    "        completion_response,\n",
    "        expectation=\"Agent LLM must produce a non-empty completion response.\",\n",
    "    )\n",
    "\n",
    "    gt_file_list = \", \".join(sorted(gt_files))\n",
    "    assess_report = kbench.assertions.assess_response_with_judge(\n",
    "        criteria=(\n",
    "            f\"The response confirms that all required files were modified or created: \"\n",
    "            f\"{gt_file_list}.\",\n",
    "            \"The response is consistent with having actually written files — it does \"\n",
    "            \"not merely describe what changes should be made without confirming they \"\n",
    "            \"were written.\",\n",
    "        ),\n",
    "        response_text=completion_response,\n",
    "        judge_llm=judge_llm,\n",
    "    )\n",
    "    for criterion_result in assess_report.results:\n",
    "        kbench.assertions.assert_true(\n",
    "            criterion_result.passed,\n",
    "            expectation=(\n",
    "                f\"Implementation completion — {criterion_result.criterion} | \"\n",
    "                f\"Judge reasoning: {criterion_result.reason}\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    print(f\"Implementation complete. Agent summary: {completion_response[:300]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Checks run after implementation, ordered cheapest → most expensive.\n",
    "`assess_response_with_judge` is used for qualitative checks that can't be\n",
    "captured by simple regex or exit codes.\n",
    "\n",
    "| # | Check | Kind | What it validates |\n",
    "|---|-------|------|-------------------|\n",
    "| 1 | **File set similarity** | Code | Agent touched roughly the right files (git-diff based) |\n",
    "| 2 | **PR-specific tests pass** | Code | New/modified test file exists and all its tests are green |\n",
    "| 3 | **No unexpected warnings** | Code | Test run is clean — no `console.warn` / `console.error` noise |\n",
    "| 4 | **Full test suite** | Code | No regressions across the entire React test suite |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_OVERLAP_THRESHOLD = 0.80\n",
    "\n",
    "\n",
    "def check_file_set_similarity(repo_path: str, gt_files: set[str]) -> None:\n",
    "    \"\"\"\n",
    "    Check 1 — File set similarity.\n",
    "\n",
    "    Uses `git diff --name-only HEAD` to determine which files the agent actually\n",
    "    modified, then measures overlap against the dynamically computed gt_files.\n",
    "    \"\"\"\n",
    "    print(\"CHECK 1: Verifying modified file set matches PR ground truth.\")\n",
    "    modified = get_git_modified_files(repo_path)\n",
    "\n",
    "    if not modified:\n",
    "        print(\"Warning: git diff returned no files. Falling back to checking gt_files exist on disk.\")\n",
    "        for f in gt_files:\n",
    "            kbench.assertions.assert_true(\n",
    "                (Path(repo_path) / f).exists(),\n",
    "                expectation=f\"Expected file not found on disk: {f}\",\n",
    "            )\n",
    "        return\n",
    "\n",
    "    intersection = gt_files.intersection(modified)\n",
    "    overlap = len(intersection) / len(gt_files)\n",
    "    kbench.assertions.assert_true(\n",
    "        overlap >= FILE_OVERLAP_THRESHOLD,\n",
    "        expectation=(\n",
    "            f\"File modification overlap should be ≥ {FILE_OVERLAP_THRESHOLD}. \"\n",
    "            f\"Got {overlap:.2f}. Missing from diff: {gt_files - modified}\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def check_pr_tests_pass(repo_path: str, pr_test_files: set[str]) -> dict[str, subprocess.CompletedProcess]:\n",
    "    \"\"\"\n",
    "    Check 2 — PR-specific tests exist and pass.\n",
    "\n",
    "    Returns test_file → CompletedProcess so check_no_unexpected_warnings can\n",
    "    reuse the results without re-running the suite.\n",
    "\n",
    "    Args:\n",
    "        pr_test_files: Subset of gt_files that are test files — computed in Phase 1.\n",
    "    \"\"\"\n",
    "    print(\"CHECK 2: Verifying PR-specific tests pass.\")\n",
    "    results = {}\n",
    "    for test_file in pr_test_files:\n",
    "        full_path = Path(repo_path) / test_file\n",
    "        kbench.assertions.assert_true(\n",
    "            full_path.exists(),\n",
    "            expectation=f\"PR test file must exist after implementation: {test_file}\",\n",
    "        )\n",
    "        result = run_command(f\"yarn test {test_file}\", cwd=repo_path)\n",
    "        kbench.assertions.assert_equal(\n",
    "            0,\n",
    "            result.returncode,\n",
    "            expectation=f\"All tests in '{test_file}' must pass. Stderr: {result.stderr[-2000:]}\",\n",
    "        )\n",
    "        results[test_file] = result\n",
    "    return results\n",
    "\n",
    "\n",
    "def check_no_unexpected_warnings(\n",
    "    pr_test_results: dict[str, subprocess.CompletedProcess],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Check 3 — Test output is free of console noise.\n",
    "\n",
    "    Reuses CompletedProcess objects from check_pr_tests_pass — tests are NOT\n",
    "    run a second time.\n",
    "    \"\"\"\n",
    "    print(\"CHECK 3: Scanning test output for unexpected console warnings.\")\n",
    "    for test_file, result in pr_test_results.items():\n",
    "        kbench.assertions.assert_not_contains_regex(\n",
    "            r\"console\\.warn\",\n",
    "            result.stderr,\n",
    "            expectation=f\"Test output for {test_file} must not contain unexpected console.warn calls.\",\n",
    "        )\n",
    "        kbench.assertions.assert_not_contains_regex(\n",
    "            r\"console\\.error\",\n",
    "            result.stderr,\n",
    "            expectation=f\"Test output for {test_file} must not contain unexpected console.error calls.\",\n",
    "        )\n",
    "\n",
    "\n",
    "def check_full_test_suite(repo_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Check 4 — Full regression suite (most expensive, runs last).\n",
    "\n",
    "    Assumes yarn install has already run in Phase 1.\n",
    "    \"\"\"\n",
    "    print(\"CHECK 4: Running full test suite (this may take several minutes).\")\n",
    "\n",
    "    build_result = run_command(\"yarn build\", cwd=repo_path)\n",
    "    kbench.assertions.assert_equal(\n",
    "        0,\n",
    "        build_result.returncode,\n",
    "        expectation=f\"yarn build must succeed. Stderr: {build_result.stderr[-2000:]}\",\n",
    "    )\n",
    "\n",
    "    test_result = run_command(\"yarn test\", cwd=repo_path, timeout=1200)\n",
    "    kbench.assertions.assert_equal(\n",
    "        0,\n",
    "        test_result.returncode,\n",
    "        expectation=f\"Full test suite must pass. Stderr: {test_result.stderr[-2000:]}\",\n",
    "    )\n",
    "    kbench.assertions.assert_not_contains_regex(\n",
    "        r\"Test Suites: \\d+ failed\",\n",
    "        test_result.stdout,\n",
    "        expectation=\"No test suites should be failing after implementation.\",\n",
    "    )\n",
    "    kbench.assertions.assert_not_contains_regex(\n",
    "        r\"snapshot.*written\",\n",
    "        test_result.stderr,\n",
    "        expectation=\"No unexpected snapshots should be written — check for unintended changes.\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Runner\n",
    "\n",
    "Phase 1 sets up the evaluation environment dynamically — using only standard git, no GitHub CLI required:\n",
    "\n",
    "1. **Clone** `facebook/react` into `./react` (skipped if already present).\n",
    "2. **`git fetch origin pull/32224/head`** + **`git checkout FETCH_HEAD`** — puts the repo at the PR tip (the *after* state, with the real changes).\n",
    "3. **`git merge-base HEAD origin/main`** — identifies the last commit before the PR diverged (the *before* state the agent will actually work from).\n",
    "4. **Diff** — computes `GT_FILES` and `pr_test_files` from the real `git diff`, capturing ground-truth file contents at the PR tip.\n",
    "5. **`git checkout <merge-base>`** — reverts to the clean before-state; the agent sees no PR changes.\n",
    "6. **`yarn install`** — installs dependencies at the base commit.\n",
    "\n",
    "This means ground-truth files and test files are always derived from the actual PR, not hardcoded. `FALLBACK_GT_FILES` in the metadata cell is documentation only — used for a sanity-check warning if the dynamic diff diverges unexpectedly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kbench.task(name=TASK_NAME)\n",
    "def react_pr_reimplementation(llm, judge_llm):\n",
    "    \"\"\"\n",
    "    Evaluates an LLM's ability to re-implement React PR #32224.\n",
    "\n",
    "    Phase 1 — Setup:\n",
    "        Clone facebook/react (if not already present), fetch the PR ref and\n",
    "        check out FETCH_HEAD to reach the PR tip (after state), capture the\n",
    "        ground-truth file contents and diff, then revert to the merge-base\n",
    "        commit so the agent works on a clean \"before\" state.  GT_FILES and\n",
    "        PR_TEST_FILES are computed from the real PR diff — not hardcoded.\n",
    "\n",
    "    Phase 2 — Analysis:    judge LLM explores the codebase and plans changes.\n",
    "    Phase 3 — Implementation: agent LLM implements changes via tool-calling loop.\n",
    "    Phase 4 — Evaluation:  code checks in increasing cost order.\n",
    "\n",
    "    Phases 2 and 3 use run_tool_loop(), which drives tool-calling manually via\n",
    "    llm.prompt() / llm.send() / llm.respond() — compatible with all providers.\n",
    "\n",
    "    Multi-model: Kaggle swaps `llm` for each model on the leaderboard automatically.\n",
    "    `judge_llm` is fixed (kbench.judge_llm) and used for plan/completion assessment.\n",
    "    \"\"\"\n",
    "    # ------------------------------------------------------------------\n",
    "    # Phase 1: Setup — clone, checkout PR, extract ground truth, revert\n",
    "    # ------------------------------------------------------------------\n",
    "    print(\"PHASE 1: Setup — cloning repo and extracting ground truth from PR.\")\n",
    "\n",
    "    # 1a. Clone the repo if it isn't already present.\n",
    "    if not os.path.isdir(REPO_PATH):\n",
    "        print(f\"Cloning {REPO_SLUG} into {REPO_PATH} …\")\n",
    "        clone_result = run_command(\n",
    "            f\"git clone https://github.com/{REPO_SLUG} {REPO_PATH}\",\n",
    "            cwd=\".\",\n",
    "            timeout=300,\n",
    "        )\n",
    "        kbench.assertions.assert_equal(\n",
    "            0,\n",
    "            clone_result.returncode,\n",
    "            expectation=f\"git clone must succeed. Stderr: {clone_result.stderr[-2000:]}\",\n",
    "        )\n",
    "\n",
    "    # 1b. Fetch the PR ref and check out FETCH_HEAD — puts the repo at the PR tip.\n",
    "    #     Uses only standard git; no GitHub CLI required.\n",
    "    print(f\"Fetching PR #{PR_NUMBER} …\")\n",
    "    fetch_result = run_command(\n",
    "        f\"git fetch origin pull/{PR_NUMBER}/head\",\n",
    "        cwd=REPO_PATH,\n",
    "        timeout=120,\n",
    "    )\n",
    "    kbench.assertions.assert_equal(\n",
    "        0,\n",
    "        fetch_result.returncode,\n",
    "        expectation=(\n",
    "            f\"git fetch origin pull/{PR_NUMBER}/head must succeed. \"\n",
    "            f\"Stderr: {fetch_result.stderr[-2000:]}\"\n",
    "        ),\n",
    "    )\n",
    "    checkout_result = run_command(\n",
    "        \"git checkout FETCH_HEAD\",\n",
    "        cwd=REPO_PATH,\n",
    "        timeout=60,\n",
    "    )\n",
    "    kbench.assertions.assert_equal(\n",
    "        0,\n",
    "        checkout_result.returncode,\n",
    "        expectation=f\"git checkout FETCH_HEAD must succeed. Stderr: {checkout_result.stderr[-2000:]}\",\n",
    "    )\n",
    "\n",
    "    # 1c. Find the merge base — the last commit on main before the PR diverged.\n",
    "    #     This is the \"before\" state the agent will work from.\n",
    "    base_result = run_command(\n",
    "        \"git merge-base HEAD origin/main\",\n",
    "        cwd=REPO_PATH,\n",
    "        timeout=30,\n",
    "    )\n",
    "    kbench.assertions.assert_equal(\n",
    "        0,\n",
    "        base_result.returncode,\n",
    "        expectation=f\"git merge-base must succeed. Stderr: {base_result.stderr[-2000:]}\",\n",
    "    )\n",
    "    merge_base_sha = base_result.stdout.strip()\n",
    "    print(f\"Merge base: {merge_base_sha}\")\n",
    "\n",
    "    # 1d. Compute GT_FILES dynamically from the actual PR diff.\n",
    "    diff_result = run_command(\n",
    "        f\"git diff {merge_base_sha} HEAD --name-only\",\n",
    "        cwd=REPO_PATH,\n",
    "        timeout=30,\n",
    "    )\n",
    "    gt_files: set[str] = {\n",
    "        line.strip()\n",
    "        for line in diff_result.stdout.splitlines()\n",
    "        if line.strip()\n",
    "    }\n",
    "    kbench.assertions.assert_true(\n",
    "        len(gt_files) > 0,\n",
    "        expectation=\"PR diff must contain at least one changed file.\",\n",
    "    )\n",
    "    print(f\"Ground-truth files ({len(gt_files)}): {sorted(gt_files)}\")\n",
    "\n",
    "    # Sanity-check: warn if the diff diverges significantly from expectations.\n",
    "    overlap = len(gt_files & FALLBACK_GT_FILES) / max(len(FALLBACK_GT_FILES), 1)\n",
    "    if overlap < 0.5:\n",
    "        print(\n",
    "            f\"Warning: dynamic GT_FILES overlaps only {overlap:.0%} with FALLBACK_GT_FILES. \"\n",
    "            \"Double-check that the correct PR was checked out.\"\n",
    "        )\n",
    "\n",
    "    # 1e. Capture the ground-truth file contents at the PR tip.\n",
    "    #     These are the actual post-PR files; we use them for evaluation later.\n",
    "    gt_contents: dict[str, str] = {}\n",
    "    for f in gt_files:\n",
    "        p = Path(REPO_PATH) / f\n",
    "        if p.exists():\n",
    "            gt_contents[f] = p.read_text(encoding=\"utf-8\")\n",
    "    print(f\"Captured ground-truth contents for {len(gt_contents)} files.\")\n",
    "\n",
    "    # 1f. Identify test files within the PR diff.\n",
    "    pr_test_files: set[str] = {\n",
    "        f for f in gt_files\n",
    "        if \"/__tests__/\" in f or f.endswith(\"-test.js\") or f.endswith(\".test.js\")\n",
    "    }\n",
    "    print(f\"PR test files: {sorted(pr_test_files)}\")\n",
    "\n",
    "    # 1g. Revert to the merge-base commit — agent now works on the \"before\" state.\n",
    "    print(f\"Reverting to merge base {merge_base_sha} …\")\n",
    "    revert_result = run_command(\n",
    "        f\"git checkout {merge_base_sha}\",\n",
    "        cwd=REPO_PATH,\n",
    "        timeout=60,\n",
    "    )\n",
    "    kbench.assertions.assert_equal(\n",
    "        0,\n",
    "        revert_result.returncode,\n",
    "        expectation=f\"Checkout to merge base must succeed. Stderr: {revert_result.stderr[-2000:]}\",\n",
    "    )\n",
    "\n",
    "    # 1h. Install dependencies at the base commit.\n",
    "    print(\"Running yarn install …\")\n",
    "    install_result = run_command(\"yarn install\", cwd=REPO_PATH, timeout=300)\n",
    "    kbench.assertions.assert_equal(\n",
    "        0,\n",
    "        install_result.returncode,\n",
    "        expectation=f\"yarn install must succeed. Stderr: {install_result.stderr[-2000:]}\",\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Phase 2: Analysis (judge LLM — codebase exploration + planning)\n",
    "    # ------------------------------------------------------------------\n",
    "    implementation_plan = run_analysis_phase(judge_llm, REPO_PATH, gt_files)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Phase 3: Implementation (agent LLM — tool-calling loop)\n",
    "    # ------------------------------------------------------------------\n",
    "    run_implementation_phase(llm, judge_llm, implementation_plan, REPO_PATH, gt_files)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Phase 4: Evaluation\n",
    "    # ------------------------------------------------------------------\n",
    "    print(\"PHASE 4: Evaluation.\")\n",
    "\n",
    "    check_file_set_similarity(REPO_PATH, gt_files)\n",
    "\n",
    "    pr_test_results = check_pr_tests_pass(REPO_PATH, pr_test_files)\n",
    "    check_no_unexpected_warnings(pr_test_results)\n",
    "\n",
    "    check_full_test_suite(REPO_PATH)\n",
    "\n",
    "\n",
    "    print(\"All checks passed.\")\n",
    "\n",
    "    )\n",
    "\n",
    "        f\"Error: {e}\"\n",
    "\n",
    "try:        f\"Task execution skipped: benchmark requires a pre-configured environment.\\n\"\n",
    "\n",
    "    react_pr_reimplementation.run(kbench.llm, judge_llm=kbench.judge_llm)    print(\n",
    "except Exception as e:"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31298,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
